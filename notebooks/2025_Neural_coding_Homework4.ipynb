{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Bernoulli Neuron: Fisher information and estimator##\n",
        "\n",
        "Assume a Bernoulli neuron with arbitrary tuning curve $f(s)$, i.e.\n",
        "$$p(r| f(s)) = f(s)^r(1- f(s))^{1-r}, \\qquad r \\in \\{0; 1\\}$$\n",
        "\n",
        "*Note: $r$ is discrete and $s$ is continuous.*\n",
        "\n",
        "**(a)** Compute the Fisher information $J_s$. *(2 points)*\n",
        "\n",
        "**(b)** Assume $f(s) = \\frac{1}{1+s^2}$. Plot the Fisher information on the interval $s\\in[-5, 5]$. Additionally, plot the Fisher information for a Poisson neuron with the same tuning curve using the formula from the lecture. *(2 points)*\n",
        "\n",
        "**(c)** Given an estimator\n",
        "$\\hat s(r) = \\begin{cases} 1 & if\\qquad r=0\\\\ 0 &if\\qquad r=1\\end{cases}$.\\\n",
        "Compute variance $\\sigma^2(s)$, bias $b(s)$, and mean squared error $MS(s)$ of this estimator. Don't use the relationship $MS(s) = b^2(s) + \\sigma^2(s)$, but instead compute them independently. *(2 points)*\n",
        "\n",
        "*Confirm for yourself with an automatized tool like wolphramalpha that their relationship is correct.*\n",
        "\n",
        "**(d)** Assume $s\\sim\\mathcal{U}([0,1])$. Compute the ideal observer $\\hat s_{MS}(r)$. *(2 points)*\n",
        "\n",
        "*Hint: Compute $p(r=0)$ and $p(r=1)$ with law of total probability. Then use Bayes' theorem to compute $p(s|r)$. Useful integrals:*\n",
        "\\begin{split}\n",
        "\\int \\frac1{1+s^2} ds &= \\tan^{-1}(s) + c, \\qquad c\\in\\mathbb{R}\\\\\n",
        "\\int \\frac{s}{1+s^2} ds &= \\frac12 \\ln(1+s^2) + c, \\qquad c\\in\\mathbb{R} \\\\\n",
        "\\int \\frac{s^2}{1+s^2} ds &= s - \\tan^{-1}(s) + c, \\qquad c\\in\\mathbb{R} \\\\\n",
        "\\int \\frac{s^3}{1+s^2} ds &= \\frac{s^2 - \\ln(1+s^2)}{2} + c, \\qquad c\\in\\mathbb{R}\n",
        "\\end{split}\n",
        "\n",
        "**(e*)** Contextualize the previous exercises with Cramer-Rao bound. *(2 points)*\n"
      ],
      "metadata": {
        "id": "uHiO-m57X0ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: Minimum Discrimination Error (MDE)\n",
        "\n",
        "Given two stimuli $s_1$ and $s_2$ with probabilities $\\lambda$ and $1-\\lambda$, the response function is given as\n",
        "\n",
        "$$p(r|s) = \\begin{cases}\n",
        "\\tfrac12 &for & s=0 &and &r\\in[-1,1]\\\\\n",
        "\\tfrac1{2x} &for &s=1 &and &r\\in[-x, x]\\\\\n",
        "0 &else\n",
        "\\end{cases}$$\n",
        "\n",
        "**(a)** Draw diagrams for each possible case and highlight the MDE. Additionally, write down the decision for the Maximal a posteriori estimator $\\hat{s}_{MAP}(r)$. *(2 points)*\\\n",
        "*Hint: Differentiate the cases by $\\lambda$ and $x$. (there are 4 possibilities)*\n",
        "\n",
        "**(b)** Compute the MDE for each case analytically. *(1 point)*\n",
        "\n",
        "**(c)** Confirm your results by simulation for multiple sets of parameters $\\lambda$ and $x$.\n",
        "Choose your selection of parameters such that they cover all cases from (a). *(2 points)*\n",
        "1. Generate stimuli (sufficient amount)\n",
        "2. Generate responses\n",
        "3. Apply estimator (write a function of $\\hat{s}_{MAP}(r)$ from (a))\n",
        "4. Compute MDE\n",
        "5. Compare to analytical MDE"
      ],
      "metadata": {
        "id": "S_nk-xSBjYux"
      }
    }
  ]
}